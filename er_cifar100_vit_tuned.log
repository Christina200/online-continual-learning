Namespace(num_runs=1, seed=0, val_size=0.1, num_val=3, num_runs_val=3, error_analysis=False, verbose=True, store=False, save_path=None, agent='ER', update='random', retrieve='random', optimizer='Adam', learning_rate=0.0001, epoch=1, batch=4, test_batch=128, weight_decay=0.0001, num_tasks=10, fix_order=True, plot_sample=False, data='cifar100', cl_type='nc', ns_factor=(0.0, 0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8, 3.2, 3.6), ns_type='noise', ns_task=(1, 1, 2, 2, 2, 2), online=True, mem_size=1000, eps_mem_batch=10, lambda_=100, alpha=0.9, fisher_update_after=50, subsample=50, gss_mem_strength=10, gss_batch_size=10, k=5, aser_type='asvm', n_smp_cls=2.0, stm_capacity=1000, classifier_chill=0.01, log_alpha=-300, minlr=0.0005, clip=10.0, mem_epoch=70, labels_trick=False, separated_softmax=False, kd_trick=False, kd_trick_star=False, review_trick=False, ncm_trick=False, mem_iters=1, min_delta=0.0, patience=0, cumulative_delta=False, temp=0.07, buffer_tracker=False, warmup=4, head='mlp', cuda=True)
Setting up data stream
Files already downloaded and verified
Files already downloaded and verified
data setup time: 0.938396692276001
Task: 0, Labels:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Task: 1, Labels:[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
Task: 2, Labels:[20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
Task: 3, Labels:[30, 31, 32, 33, 34, 35, 36, 37, 38, 39]
Task: 4, Labels:[40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
Task: 5, Labels:[50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
Task: 6, Labels:[60, 61, 62, 63, 64, 65, 66, 67, 68, 69]
Task: 7, Labels:[70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
Task: 8, Labels:[80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
Task: 9, Labels:[90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
buffer has 1000 slots
-----------run 0 training batch 0-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 3.993904, running train acc: 0.250
==>>> it: 1, mem avg. loss: 1.072056, running mem acc: 1.000
==>>> it: 101, avg. loss: 1.165198, running train acc: 0.652
==>>> it: 101, mem avg. loss: 0.416314, running mem acc: 0.876
==>>> it: 201, avg. loss: 0.917377, running train acc: 0.726
==>>> it: 201, mem avg. loss: 0.311428, running mem acc: 0.908
==>>> it: 301, avg. loss: 0.804233, running train acc: 0.762
==>>> it: 301, mem avg. loss: 0.281993, running mem acc: 0.918
==>>> it: 401, avg. loss: 0.708698, running train acc: 0.792
==>>> it: 401, mem avg. loss: 0.260293, running mem acc: 0.924
==>>> it: 501, avg. loss: 0.658954, running train acc: 0.804
==>>> it: 501, mem avg. loss: 0.250748, running mem acc: 0.925
==>>> it: 601, avg. loss: 0.626428, running train acc: 0.812
==>>> it: 601, mem avg. loss: 0.240092, running mem acc: 0.928
==>>> it: 701, avg. loss: 0.595519, running train acc: 0.821
==>>> it: 701, mem avg. loss: 0.225892, running mem acc: 0.932
==>>> it: 801, avg. loss: 0.562203, running train acc: 0.830
==>>> it: 801, mem avg. loss: 0.220428, running mem acc: 0.933
==>>> it: 901, avg. loss: 0.545940, running train acc: 0.835
==>>> it: 901, mem avg. loss: 0.209383, running mem acc: 0.936
==>>> it: 1001, avg. loss: 0.523633, running train acc: 0.841
==>>> it: 1001, mem avg. loss: 0.198559, running mem acc: 0.939
==>>> it: 1101, avg. loss: 0.500867, running train acc: 0.849
==>>> it: 1101, mem avg. loss: 0.189020, running mem acc: 0.943
==>>> it: 1201, avg. loss: 0.491584, running train acc: 0.850
==>>> it: 1201, mem avg. loss: 0.184472, running mem acc: 0.944
[0.867 0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
-----------run 0 training batch 1-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 10.084847, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.051009, running mem acc: 1.000
==>>> it: 101, avg. loss: 1.918336, running train acc: 0.473
==>>> it: 101, mem avg. loss: 0.565372, running mem acc: 0.833
==>>> it: 201, avg. loss: 1.529851, running train acc: 0.579
==>>> it: 201, mem avg. loss: 0.474777, running mem acc: 0.853
==>>> it: 301, avg. loss: 1.291639, running train acc: 0.640
==>>> it: 301, mem avg. loss: 0.406127, running mem acc: 0.868
==>>> it: 401, avg. loss: 1.149354, running train acc: 0.673
==>>> it: 401, mem avg. loss: 0.364410, running mem acc: 0.884
==>>> it: 501, avg. loss: 1.038163, running train acc: 0.705
==>>> it: 501, mem avg. loss: 0.330000, running mem acc: 0.896
==>>> it: 601, avg. loss: 0.965553, running train acc: 0.724
==>>> it: 601, mem avg. loss: 0.297903, running mem acc: 0.906
==>>> it: 701, avg. loss: 0.918393, running train acc: 0.737
==>>> it: 701, mem avg. loss: 0.280892, running mem acc: 0.913
==>>> it: 801, avg. loss: 0.887147, running train acc: 0.743
==>>> it: 801, mem avg. loss: 0.270290, running mem acc: 0.915
==>>> it: 901, avg. loss: 0.851587, running train acc: 0.753
==>>> it: 901, mem avg. loss: 0.260259, running mem acc: 0.919
==>>> it: 1001, avg. loss: 0.816519, running train acc: 0.762
==>>> it: 1001, mem avg. loss: 0.252872, running mem acc: 0.922
==>>> it: 1101, avg. loss: 0.787135, running train acc: 0.769
==>>> it: 1101, mem avg. loss: 0.245443, running mem acc: 0.923
==>>> it: 1201, avg. loss: 0.764716, running train acc: 0.774
==>>> it: 1201, mem avg. loss: 0.236682, running mem acc: 0.926
[0.654 0.854 0.    0.    0.    0.    0.    0.    0.    0.   ]
-----------run 0 training batch 2-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 11.923615, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.162135, running mem acc: 0.900
==>>> it: 101, avg. loss: 1.984438, running train acc: 0.500
==>>> it: 101, mem avg. loss: 0.436534, running mem acc: 0.875
==>>> it: 201, avg. loss: 1.289519, running train acc: 0.663
==>>> it: 201, mem avg. loss: 0.349294, running mem acc: 0.898
==>>> it: 301, avg. loss: 1.081999, running train acc: 0.709
==>>> it: 301, mem avg. loss: 0.312900, running mem acc: 0.906
==>>> it: 401, avg. loss: 0.966127, running train acc: 0.730
==>>> it: 401, mem avg. loss: 0.292245, running mem acc: 0.912
==>>> it: 501, avg. loss: 0.879438, running train acc: 0.754
==>>> it: 501, mem avg. loss: 0.266785, running mem acc: 0.920
==>>> it: 601, avg. loss: 0.817530, running train acc: 0.768
==>>> it: 601, mem avg. loss: 0.249989, running mem acc: 0.925
==>>> it: 701, avg. loss: 0.774091, running train acc: 0.778
==>>> it: 701, mem avg. loss: 0.234447, running mem acc: 0.930
==>>> it: 801, avg. loss: 0.741560, running train acc: 0.786
==>>> it: 801, mem avg. loss: 0.225269, running mem acc: 0.932
==>>> it: 901, avg. loss: 0.714267, running train acc: 0.792
==>>> it: 901, mem avg. loss: 0.212077, running mem acc: 0.936
==>>> it: 1001, avg. loss: 0.696006, running train acc: 0.798
==>>> it: 1001, mem avg. loss: 0.205957, running mem acc: 0.938
==>>> it: 1101, avg. loss: 0.669287, running train acc: 0.806
==>>> it: 1101, mem avg. loss: 0.195619, running mem acc: 0.941
==>>> it: 1201, avg. loss: 0.645046, running train acc: 0.812
==>>> it: 1201, mem avg. loss: 0.188019, running mem acc: 0.944
[0.54  0.551 0.817 0.    0.    0.    0.    0.    0.    0.   ]
-----------run 0 training batch 3-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 10.084080, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.140129, running mem acc: 0.950
==>>> it: 101, avg. loss: 1.910291, running train acc: 0.527
==>>> it: 101, mem avg. loss: 0.509809, running mem acc: 0.865
==>>> it: 201, avg. loss: 1.409164, running train acc: 0.619
==>>> it: 201, mem avg. loss: 0.406186, running mem acc: 0.885
==>>> it: 301, avg. loss: 1.203816, running train acc: 0.669
==>>> it: 301, mem avg. loss: 0.348375, running mem acc: 0.899
==>>> it: 401, avg. loss: 1.075010, running train acc: 0.701
==>>> it: 401, mem avg. loss: 0.314880, running mem acc: 0.907
==>>> it: 501, avg. loss: 0.994146, running train acc: 0.719
==>>> it: 501, mem avg. loss: 0.294333, running mem acc: 0.913
==>>> it: 601, avg. loss: 0.933390, running train acc: 0.731
==>>> it: 601, mem avg. loss: 0.271302, running mem acc: 0.918
==>>> it: 701, avg. loss: 0.884126, running train acc: 0.743
==>>> it: 701, mem avg. loss: 0.257940, running mem acc: 0.922
==>>> it: 801, avg. loss: 0.831826, running train acc: 0.758
==>>> it: 801, mem avg. loss: 0.241243, running mem acc: 0.926
==>>> it: 901, avg. loss: 0.809020, running train acc: 0.762
==>>> it: 901, mem avg. loss: 0.234002, running mem acc: 0.930
==>>> it: 1001, avg. loss: 0.775651, running train acc: 0.771
==>>> it: 1001, mem avg. loss: 0.220001, running mem acc: 0.934
==>>> it: 1101, avg. loss: 0.749537, running train acc: 0.779
==>>> it: 1101, mem avg. loss: 0.211650, running mem acc: 0.936
==>>> it: 1201, avg. loss: 0.725799, running train acc: 0.785
==>>> it: 1201, mem avg. loss: 0.200195, running mem acc: 0.940
[0.579 0.501 0.682 0.89  0.    0.    0.    0.    0.    0.   ]
-----------run 0 training batch 4-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 11.706007, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.010813, running mem acc: 1.000
==>>> it: 101, avg. loss: 1.986687, running train acc: 0.522
==>>> it: 101, mem avg. loss: 0.418850, running mem acc: 0.873
==>>> it: 201, avg. loss: 1.373281, running train acc: 0.641
==>>> it: 201, mem avg. loss: 0.347976, running mem acc: 0.891
==>>> it: 301, avg. loss: 1.144674, running train acc: 0.692
==>>> it: 301, mem avg. loss: 0.290813, running mem acc: 0.910
==>>> it: 401, avg. loss: 1.006385, running train acc: 0.719
==>>> it: 401, mem avg. loss: 0.255593, running mem acc: 0.920
==>>> it: 501, avg. loss: 0.915351, running train acc: 0.740
==>>> it: 501, mem avg. loss: 0.232734, running mem acc: 0.928
==>>> it: 601, avg. loss: 0.857957, running train acc: 0.752
==>>> it: 601, mem avg. loss: 0.224991, running mem acc: 0.930
==>>> it: 701, avg. loss: 0.801433, running train acc: 0.767
==>>> it: 701, mem avg. loss: 0.211664, running mem acc: 0.934
==>>> it: 801, avg. loss: 0.749510, running train acc: 0.779
==>>> it: 801, mem avg. loss: 0.201340, running mem acc: 0.937
==>>> it: 901, avg. loss: 0.712220, running train acc: 0.789
==>>> it: 901, mem avg. loss: 0.195558, running mem acc: 0.939
==>>> it: 1001, avg. loss: 0.686642, running train acc: 0.797
==>>> it: 1001, mem avg. loss: 0.191362, running mem acc: 0.941
==>>> it: 1101, avg. loss: 0.675113, running train acc: 0.800
==>>> it: 1101, mem avg. loss: 0.187512, running mem acc: 0.942
==>>> it: 1201, avg. loss: 0.653488, running train acc: 0.806
==>>> it: 1201, mem avg. loss: 0.179922, running mem acc: 0.944
[0.471 0.379 0.477 0.504 0.884 0.    0.    0.    0.    0.   ]
-----------run 0 training batch 5-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 11.518171, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.165042, running mem acc: 0.950
==>>> it: 101, avg. loss: 1.813020, running train acc: 0.593
==>>> it: 101, mem avg. loss: 0.385065, running mem acc: 0.904
==>>> it: 201, avg. loss: 1.320695, running train acc: 0.666
==>>> it: 201, mem avg. loss: 0.295715, running mem acc: 0.919
==>>> it: 301, avg. loss: 1.103363, running train acc: 0.703
==>>> it: 301, mem avg. loss: 0.259432, running mem acc: 0.927
==>>> it: 401, avg. loss: 0.988856, running train acc: 0.718
==>>> it: 401, mem avg. loss: 0.234640, running mem acc: 0.934
==>>> it: 501, avg. loss: 0.881250, running train acc: 0.746
==>>> it: 501, mem avg. loss: 0.210683, running mem acc: 0.939
==>>> it: 601, avg. loss: 0.837179, running train acc: 0.755
==>>> it: 601, mem avg. loss: 0.194891, running mem acc: 0.944
==>>> it: 701, avg. loss: 0.790459, running train acc: 0.765
==>>> it: 701, mem avg. loss: 0.190879, running mem acc: 0.946
==>>> it: 801, avg. loss: 0.748784, running train acc: 0.775
==>>> it: 801, mem avg. loss: 0.180076, running mem acc: 0.949
==>>> it: 901, avg. loss: 0.722450, running train acc: 0.780
==>>> it: 901, mem avg. loss: 0.171226, running mem acc: 0.952
==>>> it: 1001, avg. loss: 0.694488, running train acc: 0.787
==>>> it: 1001, mem avg. loss: 0.164448, running mem acc: 0.953
==>>> it: 1101, avg. loss: 0.682039, running train acc: 0.788
==>>> it: 1101, mem avg. loss: 0.159032, running mem acc: 0.955
==>>> it: 1201, avg. loss: 0.663516, running train acc: 0.792
==>>> it: 1201, mem avg. loss: 0.152287, running mem acc: 0.957
[0.341 0.36  0.458 0.302 0.497 0.848 0.    0.    0.    0.   ]
-----------run 0 training batch 6-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 12.152781, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.050644, running mem acc: 1.000
==>>> it: 101, avg. loss: 1.899187, running train acc: 0.515
==>>> it: 101, mem avg. loss: 0.426649, running mem acc: 0.899
==>>> it: 201, avg. loss: 1.367803, running train acc: 0.621
==>>> it: 201, mem avg. loss: 0.350338, running mem acc: 0.908
==>>> it: 301, avg. loss: 1.163326, running train acc: 0.663
==>>> it: 301, mem avg. loss: 0.301020, running mem acc: 0.920
==>>> it: 401, avg. loss: 1.020083, running train acc: 0.703
==>>> it: 401, mem avg. loss: 0.270370, running mem acc: 0.925
==>>> it: 501, avg. loss: 0.938751, running train acc: 0.726
==>>> it: 501, mem avg. loss: 0.243664, running mem acc: 0.933
==>>> it: 601, avg. loss: 0.861938, running train acc: 0.748
==>>> it: 601, mem avg. loss: 0.217152, running mem acc: 0.940
==>>> it: 701, avg. loss: 0.803847, running train acc: 0.761
==>>> it: 701, mem avg. loss: 0.206097, running mem acc: 0.943
==>>> it: 801, avg. loss: 0.756779, running train acc: 0.775
==>>> it: 801, mem avg. loss: 0.199327, running mem acc: 0.945
==>>> it: 901, avg. loss: 0.734518, running train acc: 0.781
==>>> it: 901, mem avg. loss: 0.191163, running mem acc: 0.947
==>>> it: 1001, avg. loss: 0.709143, running train acc: 0.787
==>>> it: 1001, mem avg. loss: 0.185495, running mem acc: 0.948
==>>> it: 1101, avg. loss: 0.687120, running train acc: 0.793
==>>> it: 1101, mem avg. loss: 0.177167, running mem acc: 0.951
==>>> it: 1201, avg. loss: 0.667450, running train acc: 0.797
==>>> it: 1201, mem avg. loss: 0.168941, running mem acc: 0.953
[0.411 0.306 0.43  0.393 0.478 0.486 0.863 0.    0.    0.   ]
-----------run 0 training batch 7-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 11.828641, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.026409, running mem acc: 1.000
==>>> it: 101, avg. loss: 1.891822, running train acc: 0.556
==>>> it: 101, mem avg. loss: 0.379471, running mem acc: 0.906
==>>> it: 201, avg. loss: 1.382326, running train acc: 0.630
==>>> it: 201, mem avg. loss: 0.319285, running mem acc: 0.913
==>>> it: 301, avg. loss: 1.199610, running train acc: 0.666
==>>> it: 301, mem avg. loss: 0.267012, running mem acc: 0.926
==>>> it: 401, avg. loss: 1.042810, running train acc: 0.704
==>>> it: 401, mem avg. loss: 0.230738, running mem acc: 0.934
==>>> it: 501, avg. loss: 0.958686, running train acc: 0.720
==>>> it: 501, mem avg. loss: 0.207490, running mem acc: 0.941
==>>> it: 601, avg. loss: 0.889482, running train acc: 0.738
==>>> it: 601, mem avg. loss: 0.203230, running mem acc: 0.943
==>>> it: 701, avg. loss: 0.844743, running train acc: 0.751
==>>> it: 701, mem avg. loss: 0.191420, running mem acc: 0.946
==>>> it: 801, avg. loss: 0.819515, running train acc: 0.756
==>>> it: 801, mem avg. loss: 0.186849, running mem acc: 0.947
==>>> it: 901, avg. loss: 0.783651, running train acc: 0.767
==>>> it: 901, mem avg. loss: 0.176864, running mem acc: 0.950
==>>> it: 1001, avg. loss: 0.751076, running train acc: 0.777
==>>> it: 1001, mem avg. loss: 0.172470, running mem acc: 0.950
==>>> it: 1101, avg. loss: 0.741400, running train acc: 0.779
==>>> it: 1101, mem avg. loss: 0.169332, running mem acc: 0.951
==>>> it: 1201, avg. loss: 0.721113, running train acc: 0.784
==>>> it: 1201, mem avg. loss: 0.164403, running mem acc: 0.953
[0.367 0.304 0.326 0.378 0.355 0.456 0.408 0.848 0.    0.   ]
-----------run 0 training batch 8-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 10.962324, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.003648, running mem acc: 1.000
==>>> it: 101, avg. loss: 1.964701, running train acc: 0.478
==>>> it: 101, mem avg. loss: 0.374011, running mem acc: 0.917
==>>> it: 201, avg. loss: 1.391985, running train acc: 0.611
==>>> it: 201, mem avg. loss: 0.278209, running mem acc: 0.932
==>>> it: 301, avg. loss: 1.164674, running train acc: 0.666
==>>> it: 301, mem avg. loss: 0.237922, running mem acc: 0.937
/home/xw6956/anaconda3/envs/lora_vit/lib/python3.10/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/home/xw6956/anaconda3/envs/lora_vit/lib/python3.10/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
==>>> it: 401, avg. loss: 1.024167, running train acc: 0.705
==>>> it: 401, mem avg. loss: 0.222604, running mem acc: 0.941
==>>> it: 501, avg. loss: 0.948695, running train acc: 0.724
==>>> it: 501, mem avg. loss: 0.203515, running mem acc: 0.946
==>>> it: 601, avg. loss: 0.853657, running train acc: 0.748
==>>> it: 601, mem avg. loss: 0.184808, running mem acc: 0.951
==>>> it: 701, avg. loss: 0.810758, running train acc: 0.761
==>>> it: 701, mem avg. loss: 0.175103, running mem acc: 0.953
==>>> it: 801, avg. loss: 0.765498, running train acc: 0.771
==>>> it: 801, mem avg. loss: 0.165157, running mem acc: 0.956
==>>> it: 901, avg. loss: 0.715776, running train acc: 0.784
==>>> it: 901, mem avg. loss: 0.155010, running mem acc: 0.959
==>>> it: 1001, avg. loss: 0.691564, running train acc: 0.791
==>>> it: 1001, mem avg. loss: 0.147605, running mem acc: 0.961
==>>> it: 1101, avg. loss: 0.672135, running train acc: 0.795
==>>> it: 1101, mem avg. loss: 0.145099, running mem acc: 0.961
==>>> it: 1201, avg. loss: 0.654917, running train acc: 0.799
==>>> it: 1201, mem avg. loss: 0.144014, running mem acc: 0.961
[0.294 0.211 0.396 0.233 0.22  0.363 0.425 0.438 0.877 0.   ]
-----------run 0 training batch 9-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 10.393431, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.017407, running mem acc: 1.000
==>>> it: 101, avg. loss: 1.686684, running train acc: 0.637
==>>> it: 101, mem avg. loss: 0.418156, running mem acc: 0.900
==>>> it: 201, avg. loss: 1.177209, running train acc: 0.720
==>>> it: 201, mem avg. loss: 0.342237, running mem acc: 0.907
==>>> it: 301, avg. loss: 0.936847, running train acc: 0.773
==>>> it: 301, mem avg. loss: 0.289731, running mem acc: 0.918
==>>> it: 401, avg. loss: 0.821154, running train acc: 0.795
==>>> it: 401, mem avg. loss: 0.251595, running mem acc: 0.929
==>>> it: 501, avg. loss: 0.742287, running train acc: 0.816
==>>> it: 501, mem avg. loss: 0.228338, running mem acc: 0.935
==>>> it: 601, avg. loss: 0.673265, running train acc: 0.827
==>>> it: 601, mem avg. loss: 0.211792, running mem acc: 0.940
==>>> it: 701, avg. loss: 0.629541, running train acc: 0.835
==>>> it: 701, mem avg. loss: 0.201299, running mem acc: 0.942
==>>> it: 801, avg. loss: 0.611569, running train acc: 0.838
==>>> it: 801, mem avg. loss: 0.198423, running mem acc: 0.942
==>>> it: 901, avg. loss: 0.595281, running train acc: 0.840
==>>> it: 901, mem avg. loss: 0.191226, running mem acc: 0.944
==>>> it: 1001, avg. loss: 0.579193, running train acc: 0.842
==>>> it: 1001, mem avg. loss: 0.180704, running mem acc: 0.947
==>>> it: 1101, avg. loss: 0.564469, running train acc: 0.845
==>>> it: 1101, mem avg. loss: 0.173579, running mem acc: 0.949
==>>> it: 1201, avg. loss: 0.542446, running train acc: 0.849
==>>> it: 1201, mem avg. loss: 0.164159, running mem acc: 0.952
[0.241 0.2   0.305 0.1   0.213 0.225 0.329 0.288 0.382 0.897]
-----------run 0-----------avg_end_acc 0.31799999999999995-----------train time 921.2476716041565
----------- Total 1 run: 922.1861100196838s -----------
----------- Avg_End_Acc (0.31799999999999995, nan) Avg_End_Fgt (0.5465, nan) Avg_Acc (0.5544027777777777, nan) Avg_Bwtp (0.0, nan) Avg_Fwt (0.0, nan)-----------
