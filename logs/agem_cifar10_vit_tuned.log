Namespace(num_runs=1, seed=0, val_size=0.1, num_val=3, num_runs_val=3, error_analysis=False, verbose=True, store=False, save_path=None, agent='AGEM', update='random', retrieve='random', optimizer='Adam', learning_rate=0.0001, epoch=1, batch=4, test_batch=128, weight_decay=0.0001, num_tasks=5, fix_order=True, plot_sample=False, data='cifar10', cl_type='nc', ns_factor=(0.0, 0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8, 3.2, 3.6), ns_type='noise', ns_task=(1, 1, 2, 2, 2, 2), online=True, mem_size=1000, eps_mem_batch=10, lambda_=100, alpha=0.9, fisher_update_after=50, subsample=50, gss_mem_strength=10, gss_batch_size=10, k=5, aser_type='asvm', n_smp_cls=2.0, stm_capacity=1000, classifier_chill=0.01, log_alpha=-300, minlr=0.0005, clip=10.0, mem_epoch=70, labels_trick=False, separated_softmax=False, kd_trick=False, kd_trick_star=False, review_trick=False, ncm_trick=False, mem_iters=1, min_delta=0.0, patience=0, cumulative_delta=False, temp=0.07, buffer_tracker=False, warmup=4, head='mlp', cuda=True)
Setting up data stream
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar10/cifar-10-python.tar.gz
  0%|          | 0/170498071 [00:00<?, ?it/s]  0%|          | 98304/170498071 [00:00<03:14, 877568.66it/s]  0%|          | 819200/170498071 [00:00<00:41, 4103793.93it/s]  1%|          | 1703936/170498071 [00:00<00:29, 5775004.80it/s]  2%|▏         | 2621440/170498071 [00:00<00:25, 6679769.12it/s]  2%|▏         | 3538944/170498071 [00:00<00:23, 7178526.61it/s]  3%|▎         | 4489216/170498071 [00:00<00:21, 7569960.62it/s]  3%|▎         | 5472256/170498071 [00:00<00:20, 7900755.02it/s]  4%|▍         | 6455296/170498071 [00:00<00:20, 8143608.49it/s]  4%|▍         | 7471104/170498071 [00:01<00:19, 8364748.49it/s]  5%|▍         | 8486912/170498071 [00:01<00:19, 8526350.61it/s]  6%|▌         | 9502720/170498071 [00:01<00:18, 8659642.03it/s]  6%|▌         | 10551296/170498071 [00:01<00:18, 8810200.71it/s]  7%|▋         | 11632640/170498071 [00:01<00:17, 8988311.44it/s]  7%|▋         | 12681216/170498071 [00:01<00:17, 9095463.71it/s]  8%|▊         | 13795328/170498071 [00:01<00:16, 9245568.04it/s]  9%|▊         | 14909440/170498071 [00:01<00:16, 9371810.83it/s]  9%|▉         | 16023552/170498071 [00:01<00:16, 9503356.28it/s] 10%|█         | 17137664/170498071 [00:02<00:15, 9610632.55it/s] 11%|█         | 18284544/170498071 [00:02<00:15, 9714722.30it/s] 11%|█▏        | 19431424/170498071 [00:02<00:15, 9799308.35it/s] 12%|█▏        | 20578304/170498071 [00:02<00:15, 9932108.09it/s] 13%|█▎        | 21757952/170498071 [00:02<00:14, 9988799.09it/s] 13%|█▎        | 22937600/170498071 [00:02<00:14, 10079462.16it/s] 14%|█▍        | 24117248/170498071 [00:02<00:14, 10147498.41it/s] 15%|█▍        | 25296896/170498071 [00:02<00:14, 10217117.17it/s] 16%|█▌        | 26509312/170498071 [00:02<00:13, 10342176.20it/s] 16%|█▌        | 27688960/170498071 [00:03<00:13, 10448953.92it/s] 17%|█▋        | 28901376/170498071 [00:03<00:13, 10510707.74it/s] 18%|█▊        | 30113792/170498071 [00:03<00:13, 10606458.96it/s] 18%|█▊        | 31358976/170498071 [00:03<00:13, 10603753.67it/s] 19%|█▉        | 32571392/170498071 [00:03<00:13, 10596920.85it/s] 20%|█▉        | 33816576/170498071 [00:03<00:12, 10635573.12it/s] 21%|██        | 35028992/170498071 [00:03<00:12, 10816644.91it/s] 21%|██▏       | 36274176/170498071 [00:03<00:12, 10730415.08it/s] 22%|██▏       | 37519360/170498071 [00:03<00:12, 10904427.59it/s] 23%|██▎       | 38764544/170498071 [00:04<00:12, 10719740.67it/s] 23%|██▎       | 40009728/170498071 [00:04<00:12, 10784221.87it/s] 24%|██▍       | 41254912/170498071 [00:04<00:11, 11016114.89it/s] 25%|██▍       | 42500096/170498071 [00:04<00:11, 10876668.60it/s] 26%|██▌       | 43778048/170498071 [00:04<00:11, 10778151.28it/s] 26%|██▋       | 45023232/170498071 [00:04<00:11, 10941013.79it/s] 27%|██▋       | 46268416/170498071 [00:04<00:11, 10932812.89it/s] 28%|██▊       | 47546368/170498071 [00:04<00:11, 10858298.06it/s] 29%|██▊       | 48791552/170498071 [00:05<00:11, 10993901.89it/s] 29%|██▉       | 50069504/170498071 [00:05<00:11, 10910016.59it/s] 30%|███       | 51314688/170498071 [00:05<00:10, 11054343.38it/s] 31%|███       | 52559872/170498071 [00:05<00:10, 11113278.75it/s] 32%|███▏      | 53805056/170498071 [00:05<00:10, 11048551.91it/s] 32%|███▏      | 55050240/170498071 [00:05<00:10, 11006314.67it/s] 33%|███▎      | 56328192/170498071 [00:05<00:10, 10947588.95it/s] 34%|███▍      | 57573376/170498071 [00:05<00:10, 11008579.53it/s] 34%|███▍      | 58818560/170498071 [00:05<00:10, 10931299.35it/s] 35%|███▌      | 60096512/170498071 [00:06<00:10, 10916077.28it/s] 36%|███▌      | 61341696/170498071 [00:06<00:09, 10964542.77it/s] 37%|███▋      | 62554112/170498071 [00:06<00:09, 11233674.13it/s] 37%|███▋      | 63700992/170498071 [00:06<00:09, 10998882.93it/s] 38%|███▊      | 64815104/170498071 [00:06<00:09, 10777541.61it/s] 39%|███▊      | 65961984/170498071 [00:06<00:09, 10869483.30it/s] 39%|███▉      | 67076096/170498071 [00:06<00:09, 10930353.07it/s] 40%|███▉      | 68190208/170498071 [00:06<00:09, 10805818.37it/s] 41%|████      | 69304320/170498071 [00:06<00:09, 10900496.89it/s] 41%|████▏     | 70483968/170498071 [00:06<00:09, 10998264.08it/s] 42%|████▏     | 71598080/170498071 [00:07<00:09, 10978401.70it/s] 43%|████▎     | 72744960/170498071 [00:07<00:08, 10937255.68it/s] 43%|████▎     | 73957376/170498071 [00:07<00:08, 11189094.67it/s] 44%|████▍     | 75104256/170498071 [00:07<00:08, 11194476.52it/s] 45%|████▍     | 76251136/170498071 [00:07<00:08, 11194062.25it/s] 45%|████▌     | 77398016/170498071 [00:07<00:08, 11269101.71it/s] 46%|████▌     | 78544896/170498071 [00:07<00:08, 11300008.35it/s] 47%|████▋     | 79724544/170498071 [00:07<00:07, 11429396.89it/s] 47%|████▋     | 80936960/170498071 [00:07<00:07, 11528326.78it/s] 48%|████▊     | 82116608/170498071 [00:08<00:07, 11428134.09it/s] 49%|████▉     | 83296256/170498071 [00:08<00:07, 11474976.06it/s] 50%|████▉     | 84508672/170498071 [00:08<00:07, 11493320.70it/s] 50%|█████     | 85753856/170498071 [00:08<00:07, 11767519.58it/s] 51%|█████     | 86933504/170498071 [00:08<00:07, 11681942.23it/s] 52%|█████▏    | 88113152/170498071 [00:08<00:07, 11704557.93it/s] 52%|█████▏    | 89391104/170498071 [00:08<00:06, 12008991.03it/s] 53%|█████▎    | 90603520/170498071 [00:08<00:06, 11821583.89it/s] 54%|█████▍    | 91815936/170498071 [00:08<00:06, 11809370.75it/s] 55%|█████▍    | 93093888/170498071 [00:08<00:06, 12085433.76it/s] 55%|█████▌    | 94306304/170498071 [00:09<00:06, 12000463.30it/s] 56%|█████▌    | 95518720/170498071 [00:09<00:06, 12029919.48it/s] 57%|█████▋    | 96763904/170498071 [00:09<00:06, 12103706.62it/s] 57%|█████▋    | 98009088/170498071 [00:09<00:05, 12129452.67it/s] 58%|█████▊    | 99287040/170498071 [00:09<00:05, 12180208.05it/s] 59%|█████▉    | 100532224/170498071 [00:09<00:05, 12236137.70it/s] 60%|█████▉    | 101777408/170498071 [00:09<00:05, 12253030.93it/s] 60%|██████    | 103088128/170498071 [00:09<00:05, 12438523.30it/s] 61%|██████    | 104366080/170498071 [00:09<00:05, 12470662.49it/s] 62%|██████▏   | 105644032/170498071 [00:09<00:05, 12402769.17it/s] 63%|██████▎   | 106987520/170498071 [00:10<00:05, 12592743.52it/s] 64%|██████▎   | 108363776/170498071 [00:10<00:04, 12862808.92it/s] 64%|██████▍   | 109674496/170498071 [00:10<00:04, 12780681.79it/s] 65%|██████▌   | 110985216/170498071 [00:10<00:04, 12657255.66it/s] 66%|██████▌   | 112328704/170498071 [00:10<00:04, 12675294.64it/s] 67%|██████▋   | 113770496/170498071 [00:10<00:04, 12962062.25it/s] 68%|██████▊   | 115146752/170498071 [00:10<00:04, 13162251.56it/s] 68%|██████▊   | 116490240/170498071 [00:10<00:04, 12949270.94it/s] 69%|██████▉   | 117800960/170498071 [00:10<00:04, 12812241.11it/s] 70%|██████▉   | 119242752/170498071 [00:11<00:03, 13259751.99it/s] 71%|███████   | 120586240/170498071 [00:11<00:03, 13259789.07it/s] 72%|███████▏  | 121929728/170498071 [00:11<00:03, 12909225.08it/s] 72%|███████▏  | 123338752/170498071 [00:11<00:03, 13205482.04it/s] 73%|███████▎  | 124780544/170498071 [00:11<00:03, 13520213.99it/s] 74%|███████▍  | 126156800/170498071 [00:11<00:03, 13351143.07it/s] 75%|███████▍  | 127500288/170498071 [00:11<00:03, 13171886.14it/s] 76%|███████▌  | 128942080/170498071 [00:11<00:03, 13463609.64it/s] 76%|███████▋  | 130383872/170498071 [00:11<00:02, 13716244.60it/s] 77%|███████▋  | 131760128/170498071 [00:11<00:02, 13390182.80it/s] 78%|███████▊  | 133169152/170498071 [00:12<00:02, 13560741.00it/s] 79%|███████▉  | 134643712/170498071 [00:12<00:02, 13704400.07it/s] 80%|███████▉  | 136052736/170498071 [00:12<00:02, 13783376.28it/s] 81%|████████  | 137494528/170498071 [00:12<00:02, 13895403.69it/s] 81%|████████▏ | 138936320/170498071 [00:12<00:02, 14046937.37it/s] 82%|████████▏ | 140378112/170498071 [00:12<00:02, 14110994.26it/s] 83%|████████▎ | 141852672/170498071 [00:12<00:02, 14288852.67it/s] 84%|████████▍ | 143392768/170498071 [00:12<00:01, 14619508.37it/s] 85%|████████▍ | 144867328/170498071 [00:12<00:01, 14537103.22it/s] 86%|████████▌ | 146440192/170498071 [00:12<00:01, 14760308.36it/s] 87%|████████▋ | 148013056/170498071 [00:13<00:01, 15041056.12it/s] 88%|████████▊ | 149585920/170498071 [00:13<00:01, 15177680.93it/s] 89%|████████▊ | 151257088/170498071 [00:13<00:01, 15203503.33it/s] 90%|████████▉ | 152928256/170498071 [00:13<00:01, 15243111.19it/s] 91%|█████████ | 154664960/170498071 [00:13<00:01, 15805015.59it/s] 92%|█████████▏| 156368896/170498071 [00:13<00:00, 16128983.97it/s] 93%|█████████▎| 158007296/170498071 [00:13<00:00, 16132693.85it/s] 94%|█████████▎| 159776768/170498071 [00:13<00:00, 16569208.12it/s] 95%|█████████▍| 161546240/170498071 [00:13<00:00, 16884885.16it/s] 96%|█████████▌| 163315712/170498071 [00:13<00:00, 16929833.86it/s] 97%|█████████▋| 165150720/170498071 [00:14<00:00, 17325326.73it/s] 98%|█████████▊| 166985728/170498071 [00:14<00:00, 17588978.87it/s] 99%|█████████▉| 168886272/170498071 [00:14<00:00, 17911506.65it/s]100%|██████████| 170498071/170498071 [00:14<00:00, 11857194.33it/s]
Extracting ./datasets/cifar10/cifar-10-python.tar.gz to ./datasets/cifar10
Files already downloaded and verified
data setup time: 17.17126488685608
Task: 0, Labels:[0, 1]
Task: 1, Labels:[2, 3]
Task: 2, Labels:[4, 5]
Task: 3, Labels:[6, 7]
Task: 4, Labels:[8, 9]
buffer has 1000 slots
-----------run 0 training batch 0-------------
size: (10000, 32, 32, 3), (10000,)
==>>> it: 1, avg. loss: 1.744717, running train acc: 0.375
==>>> it: 101, avg. loss: 0.385176, running train acc: 0.870
==>>> it: 201, avg. loss: 0.279893, running train acc: 0.905
==>>> it: 301, avg. loss: 0.248603, running train acc: 0.916
==>>> it: 401, avg. loss: 0.210111, running train acc: 0.928
==>>> it: 501, avg. loss: 0.199162, running train acc: 0.934
==>>> it: 601, avg. loss: 0.198309, running train acc: 0.933
==>>> it: 701, avg. loss: 0.194506, running train acc: 0.932
==>>> it: 801, avg. loss: 0.185505, running train acc: 0.935
==>>> it: 901, avg. loss: 0.187004, running train acc: 0.933
==>>> it: 1001, avg. loss: 0.184747, running train acc: 0.933
==>>> it: 1101, avg. loss: 0.178660, running train acc: 0.936
==>>> it: 1201, avg. loss: 0.178730, running train acc: 0.935
==>>> it: 1301, avg. loss: 0.177590, running train acc: 0.935
==>>> it: 1401, avg. loss: 0.176561, running train acc: 0.937
==>>> it: 1501, avg. loss: 0.172942, running train acc: 0.937
==>>> it: 1601, avg. loss: 0.170395, running train acc: 0.938
==>>> it: 1701, avg. loss: 0.171164, running train acc: 0.938
==>>> it: 1801, avg. loss: 0.172031, running train acc: 0.937
==>>> it: 1901, avg. loss: 0.174293, running train acc: 0.936
==>>> it: 2001, avg. loss: 0.170056, running train acc: 0.937
==>>> it: 2101, avg. loss: 0.168786, running train acc: 0.938
==>>> it: 2201, avg. loss: 0.167279, running train acc: 0.939
==>>> it: 2301, avg. loss: 0.166520, running train acc: 0.940
==>>> it: 2401, avg. loss: 0.163964, running train acc: 0.941
[0.9715 0.     0.     0.     0.    ]
-----------run 0 training batch 1-------------
size: (10000, 32, 32, 3), (10000,)
==>>> it: 1, avg. loss: 10.406664, running train acc: 0.000
==>>> it: 101, avg. loss: 1.111030, running train acc: 0.507
==>>> it: 201, avg. loss: 0.916136, running train acc: 0.536
==>>> it: 301, avg. loss: 0.806880, running train acc: 0.593
==>>> it: 401, avg. loss: 0.747149, running train acc: 0.624
==>>> it: 501, avg. loss: 0.708103, running train acc: 0.643
==>>> it: 601, avg. loss: 0.686365, running train acc: 0.656
==>>> it: 701, avg. loss: 0.659392, running train acc: 0.672
==>>> it: 801, avg. loss: 0.636495, running train acc: 0.685
==>>> it: 901, avg. loss: 0.613173, running train acc: 0.700
==>>> it: 1001, avg. loss: 0.591569, running train acc: 0.714
==>>> it: 1101, avg. loss: 0.575849, running train acc: 0.725
==>>> it: 1201, avg. loss: 0.567809, running train acc: 0.730
==>>> it: 1301, avg. loss: 0.548932, running train acc: 0.741
==>>> it: 1401, avg. loss: 0.536456, running train acc: 0.748
==>>> it: 1501, avg. loss: 0.530679, running train acc: 0.751
==>>> it: 1601, avg. loss: 0.520557, running train acc: 0.756
==>>> it: 1701, avg. loss: 0.513541, running train acc: 0.761
==>>> it: 1801, avg. loss: 0.508075, running train acc: 0.764
==>>> it: 1901, avg. loss: 0.498957, running train acc: 0.769
==>>> it: 2001, avg. loss: 0.491292, running train acc: 0.773
==>>> it: 2101, avg. loss: 0.483460, running train acc: 0.778
==>>> it: 2201, avg. loss: 0.478504, running train acc: 0.781
==>>> it: 2301, avg. loss: 0.473259, running train acc: 0.784
==>>> it: 2401, avg. loss: 0.470161, running train acc: 0.786
[0.003 0.871 0.    0.    0.   ]
-----------run 0 training batch 2-------------
size: (10000, 32, 32, 3), (10000,)
==>>> it: 1, avg. loss: 11.123726, running train acc: 0.000
==>>> it: 101, avg. loss: 0.943819, running train acc: 0.635
==>>> it: 201, avg. loss: 0.685259, running train acc: 0.724
==>>> it: 301, avg. loss: 0.595731, running train acc: 0.760
==>>> it: 401, avg. loss: 0.535463, running train acc: 0.782
==>>> it: 501, avg. loss: 0.503442, running train acc: 0.793
==>>> it: 601, avg. loss: 0.472990, running train acc: 0.807
==>>> it: 701, avg. loss: 0.458228, running train acc: 0.815
==>>> it: 801, avg. loss: 0.437009, running train acc: 0.825
==>>> it: 901, avg. loss: 0.425150, running train acc: 0.831
==>>> it: 1001, avg. loss: 0.409438, running train acc: 0.837
==>>> it: 1101, avg. loss: 0.399531, running train acc: 0.841
==>>> it: 1201, avg. loss: 0.391506, running train acc: 0.844
==>>> it: 1301, avg. loss: 0.385835, running train acc: 0.847
==>>> it: 1401, avg. loss: 0.381246, running train acc: 0.848
==>>> it: 1501, avg. loss: 0.376395, running train acc: 0.850
==>>> it: 1601, avg. loss: 0.371956, running train acc: 0.853
==>>> it: 1701, avg. loss: 0.364478, running train acc: 0.857
==>>> it: 1801, avg. loss: 0.359601, running train acc: 0.860
==>>> it: 1901, avg. loss: 0.355577, running train acc: 0.862
==>>> it: 2001, avg. loss: 0.351329, running train acc: 0.864
==>>> it: 2101, avg. loss: 0.348581, running train acc: 0.864
==>>> it: 2201, avg. loss: 0.345295, running train acc: 0.865
==>>> it: 2301, avg. loss: 0.343591, running train acc: 0.866
==>>> it: 2401, avg. loss: 0.343292, running train acc: 0.867
[0.008  0.     0.8895 0.     0.    ]
-----------run 0 training batch 3-------------
size: (10000, 32, 32, 3), (10000,)
==>>> it: 1, avg. loss: 10.291574, running train acc: 0.000
==>>> it: 101, avg. loss: 0.869971, running train acc: 0.689
==>>> it: 201, avg. loss: 0.560893, running train acc: 0.798
==>>> it: 301, avg. loss: 0.450312, running train acc: 0.835
==>>> it: 401, avg. loss: 0.398831, running train acc: 0.857
==>>> it: 501, avg. loss: 0.362932, running train acc: 0.867
==>>> it: 601, avg. loss: 0.332770, running train acc: 0.878
==>>> it: 701, avg. loss: 0.317554, running train acc: 0.881
==>>> it: 801, avg. loss: 0.303454, running train acc: 0.886
==>>> it: 901, avg. loss: 0.283885, running train acc: 0.893
==>>> it: 1001, avg. loss: 0.272242, running train acc: 0.897
==>>> it: 1101, avg. loss: 0.265485, running train acc: 0.899
==>>> it: 1201, avg. loss: 0.255764, running train acc: 0.903
==>>> it: 1301, avg. loss: 0.245888, running train acc: 0.907
==>>> it: 1401, avg. loss: 0.243167, running train acc: 0.907
==>>> it: 1501, avg. loss: 0.239353, running train acc: 0.908
==>>> it: 1601, avg. loss: 0.233359, running train acc: 0.910
==>>> it: 1701, avg. loss: 0.228080, running train acc: 0.913
==>>> it: 1801, avg. loss: 0.227078, running train acc: 0.914
==>>> it: 1901, avg. loss: 0.221375, running train acc: 0.917
==>>> it: 2001, avg. loss: 0.218564, running train acc: 0.917
==>>> it: 2101, avg. loss: 0.214145, running train acc: 0.919
==>>> it: 2201, avg. loss: 0.208970, running train acc: 0.921
==>>> it: 2301, avg. loss: 0.205339, running train acc: 0.922
==>>> it: 2401, avg. loss: 0.201496, running train acc: 0.924
[0.2065 0.     0.     0.96   0.    ]
-----------run 0 training batch 4-------------
size: (10000, 32, 32, 3), (10000,)
==>>> it: 1, avg. loss: 8.199440, running train acc: 0.000
==>>> it: 101, avg. loss: 0.745126, running train acc: 0.703
==>>> it: 201, avg. loss: 0.536276, running train acc: 0.788
==>>> it: 301, avg. loss: 0.457116, running train acc: 0.824
==>>> it: 401, avg. loss: 0.410301, running train acc: 0.845
==>>> it: 501, avg. loss: 0.373399, running train acc: 0.861
==>>> it: 601, avg. loss: 0.348424, running train acc: 0.868
==>>> it: 701, avg. loss: 0.329628, running train acc: 0.875
==>>> it: 801, avg. loss: 0.320769, running train acc: 0.879
==>>> it: 901, avg. loss: 0.312904, running train acc: 0.881
==>>> it: 1001, avg. loss: 0.295784, running train acc: 0.887
==>>> it: 1101, avg. loss: 0.290470, running train acc: 0.890
==>>> it: 1201, avg. loss: 0.281936, running train acc: 0.894
==>>> it: 1301, avg. loss: 0.276404, running train acc: 0.897
==>>> it: 1401, avg. loss: 0.270953, running train acc: 0.899
==>>> it: 1501, avg. loss: 0.266755, running train acc: 0.900
==>>> it: 1601, avg. loss: 0.262143, running train acc: 0.902
==>>> it: 1701, avg. loss: 0.259325, running train acc: 0.904
==>>> it: 1801, avg. loss: 0.253163, running train acc: 0.906
/home/christina/anaconda3/envs/lora_vit/lib/python3.11/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/home/christina/anaconda3/envs/lora_vit/lib/python3.11/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
==>>> it: 1901, avg. loss: 0.246152, running train acc: 0.909
==>>> it: 2001, avg. loss: 0.244381, running train acc: 0.909
==>>> it: 2101, avg. loss: 0.240361, running train acc: 0.911
==>>> it: 2201, avg. loss: 0.236731, running train acc: 0.913
==>>> it: 2301, avg. loss: 0.232756, running train acc: 0.914
==>>> it: 2401, avg. loss: 0.230169, running train acc: 0.914
[0.     0.1475 0.171  0.187  0.9545]
-----------run 0-----------avg_end_acc 0.292-----------train time 1915.304220199585
----------- Total 1 run: 1932.4755654335022s -----------
----------- Avg_End_Acc (0.292, nan) Avg_End_Fgt (0.6373, nan) Avg_Acc (0.4582583333333333, nan) Avg_Bwtp (0.0, nan) Avg_Fwt (0.0, nan)-----------
