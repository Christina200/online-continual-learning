Namespace(num_runs=1, seed=0, val_size=0.1, num_val=3, num_runs_val=3, error_analysis=False, verbose=True, store=False, save_path=None, agent='ER', update='random', retrieve='MIR', optimizer='Adam', learning_rate=0.0001, epoch=1, batch=4, test_batch=128, weight_decay=0.0001, num_tasks=5, fix_order=True, plot_sample=False, data='cifar10', cl_type='nc', ns_factor=(0.0, 0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8, 3.2, 3.6), ns_type='noise', ns_task=(1, 1, 2, 2, 2, 2), online=True, mem_size=1000, eps_mem_batch=10, lambda_=100, alpha=0.9, fisher_update_after=50, subsample=25, gss_mem_strength=10, gss_batch_size=10, k=5, aser_type='asvm', n_smp_cls=2.0, stm_capacity=1000, classifier_chill=0.01, log_alpha=-300, minlr=0.0005, clip=10.0, mem_epoch=70, labels_trick=False, separated_softmax=False, kd_trick=False, kd_trick_star=False, review_trick=False, ncm_trick=False, mem_iters=1, min_delta=0.0, patience=0, cumulative_delta=False, temp=0.07, buffer_tracker=False, warmup=4, head='mlp', cuda=True)
Setting up data stream
Files already downloaded and verified
Files already downloaded and verified
data setup time: 1.2297272682189941
Task: 0, Labels:[0, 1]
Task: 1, Labels:[2, 3]
Task: 2, Labels:[4, 5]
Task: 3, Labels:[6, 7]
Task: 4, Labels:[8, 9]
buffer has 1000 slots
-----------run 0 training batch 0-------------
size: (10000, 32, 32, 3), (10000,)
==>>> it: 1, avg. loss: 1.744717, running train acc: 0.375
==>>> it: 1, mem avg. loss: 0.166591, running mem acc: 1.000
==>>> it: 101, avg. loss: 0.328041, running train acc: 0.882
==>>> it: 101, mem avg. loss: 0.061430, running mem acc: 0.981
==>>> it: 201, avg. loss: 0.222056, running train acc: 0.921
==>>> it: 201, mem avg. loss: 0.059045, running mem acc: 0.980
==>>> it: 301, avg. loss: 0.177871, running train acc: 0.935
==>>> it: 301, mem avg. loss: 0.050186, running mem acc: 0.983
==>>> it: 401, avg. loss: 0.145098, running train acc: 0.949
==>>> it: 401, mem avg. loss: 0.043299, running mem acc: 0.985
==>>> it: 501, avg. loss: 0.148697, running train acc: 0.948
==>>> it: 501, mem avg. loss: 0.045312, running mem acc: 0.984
==>>> it: 601, avg. loss: 0.145187, running train acc: 0.948
==>>> it: 601, mem avg. loss: 0.043084, running mem acc: 0.985
==>>> it: 701, avg. loss: 0.135327, running train acc: 0.952
==>>> it: 701, mem avg. loss: 0.039986, running mem acc: 0.986
==>>> it: 801, avg. loss: 0.134032, running train acc: 0.952
==>>> it: 801, mem avg. loss: 0.040796, running mem acc: 0.985
==>>> it: 901, avg. loss: 0.125124, running train acc: 0.955
==>>> it: 901, mem avg. loss: 0.042627, running mem acc: 0.984
==>>> it: 1001, avg. loss: 0.134658, running train acc: 0.952
==>>> it: 1001, mem avg. loss: 0.040861, running mem acc: 0.985
==>>> it: 1101, avg. loss: 0.128667, running train acc: 0.953
==>>> it: 1101, mem avg. loss: 0.040215, running mem acc: 0.985
==>>> it: 1201, avg. loss: 0.127246, running train acc: 0.954
==>>> it: 1201, mem avg. loss: 0.038720, running mem acc: 0.986
==>>> it: 1301, avg. loss: 0.123489, running train acc: 0.956
==>>> it: 1301, mem avg. loss: 0.037083, running mem acc: 0.987
==>>> it: 1401, avg. loss: 0.123099, running train acc: 0.956
==>>> it: 1401, mem avg. loss: 0.037632, running mem acc: 0.987
==>>> it: 1501, avg. loss: 0.120530, running train acc: 0.957
==>>> it: 1501, mem avg. loss: 0.037526, running mem acc: 0.987
==>>> it: 1601, avg. loss: 0.115938, running train acc: 0.958
==>>> it: 1601, mem avg. loss: 0.036014, running mem acc: 0.987
==>>> it: 1701, avg. loss: 0.117479, running train acc: 0.957
==>>> it: 1701, mem avg. loss: 0.035914, running mem acc: 0.987
==>>> it: 1801, avg. loss: 0.116531, running train acc: 0.958
==>>> it: 1801, mem avg. loss: 0.035307, running mem acc: 0.988
==>>> it: 1901, avg. loss: 0.117526, running train acc: 0.957
==>>> it: 1901, mem avg. loss: 0.034809, running mem acc: 0.988
==>>> it: 2001, avg. loss: 0.117710, running train acc: 0.957
==>>> it: 2001, mem avg. loss: 0.034635, running mem acc: 0.988
==>>> it: 2101, avg. loss: 0.115615, running train acc: 0.958
==>>> it: 2101, mem avg. loss: 0.034381, running mem acc: 0.988
==>>> it: 2201, avg. loss: 0.113407, running train acc: 0.959
==>>> it: 2201, mem avg. loss: 0.034628, running mem acc: 0.988
==>>> it: 2301, avg. loss: 0.114933, running train acc: 0.958
==>>> it: 2301, mem avg. loss: 0.034384, running mem acc: 0.988
==>>> it: 2401, avg. loss: 0.114335, running train acc: 0.958
==>>> it: 2401, mem avg. loss: 0.033751, running mem acc: 0.988
[0.966 0.    0.    0.    0.   ]
-----------run 0 training batch 1-------------
size: (10000, 32, 32, 3), (10000,)
==>>> it: 1, avg. loss: 10.310366, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.032289, running mem acc: 1.000
==>>> it: 101, avg. loss: 1.305865, running train acc: 0.510
==>>> it: 101, mem avg. loss: 0.642571, running mem acc: 0.814
==>>> it: 201, avg. loss: 0.986696, running train acc: 0.626
==>>> it: 201, mem avg. loss: 0.463491, running mem acc: 0.861
==>>> it: 301, avg. loss: 0.847464, running train acc: 0.669
==>>> it: 301, mem avg. loss: 0.383624, running mem acc: 0.880
==>>> it: 401, avg. loss: 0.760384, running train acc: 0.704
==>>> it: 401, mem avg. loss: 0.333678, running mem acc: 0.893
==>>> it: 501, avg. loss: 0.703862, running train acc: 0.729
==>>> it: 501, mem avg. loss: 0.292599, running mem acc: 0.906
==>>> it: 601, avg. loss: 0.665941, running train acc: 0.743
==>>> it: 601, mem avg. loss: 0.267763, running mem acc: 0.912
==>>> it: 701, avg. loss: 0.628525, running train acc: 0.757
==>>> it: 701, mem avg. loss: 0.245483, running mem acc: 0.919
==>>> it: 801, avg. loss: 0.598681, running train acc: 0.770
==>>> it: 801, mem avg. loss: 0.228668, running mem acc: 0.924
==>>> it: 901, avg. loss: 0.571429, running train acc: 0.779
==>>> it: 901, mem avg. loss: 0.213401, running mem acc: 0.929
==>>> it: 1001, avg. loss: 0.544431, running train acc: 0.792
==>>> it: 1001, mem avg. loss: 0.203545, running mem acc: 0.933
==>>> it: 1101, avg. loss: 0.527882, running train acc: 0.799
==>>> it: 1101, mem avg. loss: 0.195572, running mem acc: 0.936
==>>> it: 1201, avg. loss: 0.517181, running train acc: 0.802
==>>> it: 1201, mem avg. loss: 0.190325, running mem acc: 0.938
==>>> it: 1301, avg. loss: 0.496615, running train acc: 0.809
==>>> it: 1301, mem avg. loss: 0.182570, running mem acc: 0.941
==>>> it: 1401, avg. loss: 0.480966, running train acc: 0.815
==>>> it: 1401, mem avg. loss: 0.175241, running mem acc: 0.944
==>>> it: 1501, avg. loss: 0.474231, running train acc: 0.818
==>>> it: 1501, mem avg. loss: 0.169017, running mem acc: 0.946
==>>> it: 1601, avg. loss: 0.463445, running train acc: 0.822
==>>> it: 1601, mem avg. loss: 0.164172, running mem acc: 0.947
==>>> it: 1701, avg. loss: 0.455468, running train acc: 0.826
==>>> it: 1701, mem avg. loss: 0.159800, running mem acc: 0.949
==>>> it: 1801, avg. loss: 0.446608, running train acc: 0.829
==>>> it: 1801, mem avg. loss: 0.155420, running mem acc: 0.950
==>>> it: 1901, avg. loss: 0.438215, running train acc: 0.832
==>>> it: 1901, mem avg. loss: 0.151650, running mem acc: 0.951
==>>> it: 2001, avg. loss: 0.430743, running train acc: 0.835
==>>> it: 2001, mem avg. loss: 0.147403, running mem acc: 0.953
==>>> it: 2101, avg. loss: 0.422198, running train acc: 0.838
==>>> it: 2101, mem avg. loss: 0.144351, running mem acc: 0.954
==>>> it: 2201, avg. loss: 0.417232, running train acc: 0.841
==>>> it: 2201, mem avg. loss: 0.140929, running mem acc: 0.955
==>>> it: 2301, avg. loss: 0.413145, running train acc: 0.842
==>>> it: 2301, mem avg. loss: 0.138872, running mem acc: 0.956
==>>> it: 2401, avg. loss: 0.408464, running train acc: 0.844
==>>> it: 2401, mem avg. loss: 0.136759, running mem acc: 0.957
[0.8495 0.88   0.     0.     0.    ]
-----------run 0 training batch 2-------------
size: (10000, 32, 32, 3), (10000,)
==>>> it: 1, avg. loss: 11.477745, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.109054, running mem acc: 0.950
==>>> it: 101, avg. loss: 1.185339, running train acc: 0.603
==>>> it: 101, mem avg. loss: 0.583253, running mem acc: 0.784
==>>> it: 201, avg. loss: 0.901180, running train acc: 0.687
==>>> it: 201, mem avg. loss: 0.466712, running mem acc: 0.825
==>>> it: 301, avg. loss: 0.808312, running train acc: 0.719
==>>> it: 301, mem avg. loss: 0.411126, running mem acc: 0.844
==>>> it: 401, avg. loss: 0.730111, running train acc: 0.749
==>>> it: 401, mem avg. loss: 0.367314, running mem acc: 0.863
==>>> it: 501, avg. loss: 0.671331, running train acc: 0.765
==>>> it: 501, mem avg. loss: 0.337442, running mem acc: 0.877
==>>> it: 601, avg. loss: 0.631689, running train acc: 0.779
==>>> it: 601, mem avg. loss: 0.311770, running mem acc: 0.887
==>>> it: 701, avg. loss: 0.597530, running train acc: 0.792
==>>> it: 701, mem avg. loss: 0.291361, running mem acc: 0.894
==>>> it: 801, avg. loss: 0.565679, running train acc: 0.801
==>>> it: 801, mem avg. loss: 0.277878, running mem acc: 0.900
==>>> it: 901, avg. loss: 0.540951, running train acc: 0.810
==>>> it: 901, mem avg. loss: 0.265490, running mem acc: 0.905
==>>> it: 1001, avg. loss: 0.516673, running train acc: 0.819
==>>> it: 1001, mem avg. loss: 0.251485, running mem acc: 0.910
==>>> it: 1101, avg. loss: 0.498996, running train acc: 0.824
==>>> it: 1101, mem avg. loss: 0.242437, running mem acc: 0.913
==>>> it: 1201, avg. loss: 0.475274, running train acc: 0.831
==>>> it: 1201, mem avg. loss: 0.229593, running mem acc: 0.918
==>>> it: 1301, avg. loss: 0.464046, running train acc: 0.835
==>>> it: 1301, mem avg. loss: 0.220654, running mem acc: 0.922
==>>> it: 1401, avg. loss: 0.453947, running train acc: 0.839
==>>> it: 1401, mem avg. loss: 0.214562, running mem acc: 0.924
==>>> it: 1501, avg. loss: 0.443873, running train acc: 0.842
==>>> it: 1501, mem avg. loss: 0.210374, running mem acc: 0.926
==>>> it: 1601, avg. loss: 0.437182, running train acc: 0.844
==>>> it: 1601, mem avg. loss: 0.206466, running mem acc: 0.928
==>>> it: 1701, avg. loss: 0.420789, running train acc: 0.850
==>>> it: 1701, mem avg. loss: 0.197865, running mem acc: 0.931
==>>> it: 1801, avg. loss: 0.412508, running train acc: 0.853
==>>> it: 1801, mem avg. loss: 0.193747, running mem acc: 0.933
==>>> it: 1901, avg. loss: 0.407825, running train acc: 0.855
==>>> it: 1901, mem avg. loss: 0.189051, running mem acc: 0.935
==>>> it: 2001, avg. loss: 0.398015, running train acc: 0.858
==>>> it: 2001, mem avg. loss: 0.183427, running mem acc: 0.937
==>>> it: 2101, avg. loss: 0.391924, running train acc: 0.860
==>>> it: 2101, mem avg. loss: 0.178570, running mem acc: 0.939
==>>> it: 2201, avg. loss: 0.385838, running train acc: 0.862
==>>> it: 2201, mem avg. loss: 0.174976, running mem acc: 0.940
==>>> it: 2301, avg. loss: 0.380387, running train acc: 0.865
==>>> it: 2301, mem avg. loss: 0.170573, running mem acc: 0.941
==>>> it: 2401, avg. loss: 0.374593, running train acc: 0.867
==>>> it: 2401, mem avg. loss: 0.165973, running mem acc: 0.943
[0.8445 0.3605 0.909  0.     0.    ]
-----------run 0 training batch 3-------------
size: (10000, 32, 32, 3), (10000,)
==>>> it: 1, avg. loss: 10.430120, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.034260, running mem acc: 1.000
==>>> it: 101, avg. loss: 0.983832, running train acc: 0.752
==>>> it: 101, mem avg. loss: 0.505212, running mem acc: 0.834
==>>> it: 201, avg. loss: 0.666272, running train acc: 0.818
==>>> it: 201, mem avg. loss: 0.383555, running mem acc: 0.873
==>>> it: 301, avg. loss: 0.532062, running train acc: 0.849
==>>> it: 301, mem avg. loss: 0.322780, running mem acc: 0.892
==>>> it: 401, avg. loss: 0.471277, running train acc: 0.862
==>>> it: 401, mem avg. loss: 0.286094, running mem acc: 0.903
==>>> it: 501, avg. loss: 0.412684, running train acc: 0.876
==>>> it: 501, mem avg. loss: 0.254173, running mem acc: 0.914
==>>> it: 601, avg. loss: 0.382430, running train acc: 0.885
==>>> it: 601, mem avg. loss: 0.233033, running mem acc: 0.922
==>>> it: 701, avg. loss: 0.365199, running train acc: 0.888
==>>> it: 701, mem avg. loss: 0.224877, running mem acc: 0.924
==>>> it: 801, avg. loss: 0.350432, running train acc: 0.892
==>>> it: 801, mem avg. loss: 0.212096, running mem acc: 0.929
==>>> it: 901, avg. loss: 0.333021, running train acc: 0.896
==>>> it: 901, mem avg. loss: 0.198484, running mem acc: 0.933
==>>> it: 1001, avg. loss: 0.315238, running train acc: 0.901
==>>> it: 1001, mem avg. loss: 0.187391, running mem acc: 0.937
==>>> it: 1101, avg. loss: 0.300166, running train acc: 0.905
==>>> it: 1101, mem avg. loss: 0.179172, running mem acc: 0.939
==>>> it: 1201, avg. loss: 0.289635, running train acc: 0.907
==>>> it: 1201, mem avg. loss: 0.174602, running mem acc: 0.941
==>>> it: 1301, avg. loss: 0.280815, running train acc: 0.910
==>>> it: 1301, mem avg. loss: 0.168371, running mem acc: 0.943
==>>> it: 1401, avg. loss: 0.277943, running train acc: 0.910
==>>> it: 1401, mem avg. loss: 0.168952, running mem acc: 0.943
==>>> it: 1501, avg. loss: 0.270745, running train acc: 0.913
==>>> it: 1501, mem avg. loss: 0.164065, running mem acc: 0.945
==>>> it: 1601, avg. loss: 0.266605, running train acc: 0.915
==>>> it: 1601, mem avg. loss: 0.158113, running mem acc: 0.947
==>>> it: 1701, avg. loss: 0.258615, running train acc: 0.917
==>>> it: 1701, mem avg. loss: 0.152089, running mem acc: 0.949
==>>> it: 1801, avg. loss: 0.251612, running train acc: 0.920
==>>> it: 1801, mem avg. loss: 0.145274, running mem acc: 0.951
==>>> it: 1901, avg. loss: 0.244362, running train acc: 0.921
==>>> it: 1901, mem avg. loss: 0.142336, running mem acc: 0.952
==>>> it: 2001, avg. loss: 0.240624, running train acc: 0.922
==>>> it: 2001, mem avg. loss: 0.138776, running mem acc: 0.954
==>>> it: 2101, avg. loss: 0.233865, running train acc: 0.924
==>>> it: 2101, mem avg. loss: 0.135832, running mem acc: 0.955
==>>> it: 2201, avg. loss: 0.229876, running train acc: 0.926
==>>> it: 2201, mem avg. loss: 0.132559, running mem acc: 0.956
==>>> it: 2301, avg. loss: 0.223935, running train acc: 0.928
==>>> it: 2301, mem avg. loss: 0.129199, running mem acc: 0.957
==>>> it: 2401, avg. loss: 0.221024, running train acc: 0.929
==>>> it: 2401, mem avg. loss: 0.126983, running mem acc: 0.958
[0.86   0.3315 0.4745 0.967  0.    ]
-----------run 0 training batch 4-------------
size: (10000, 32, 32, 3), (10000,)
==>>> it: 1, avg. loss: 9.913370, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.001326, running mem acc: 1.000
==>>> it: 101, avg. loss: 1.039816, running train acc: 0.686
==>>> it: 101, mem avg. loss: 0.486744, running mem acc: 0.822
==>>> it: 201, avg. loss: 0.709333, running train acc: 0.776
==>>> it: 201, mem avg. loss: 0.323424, running mem acc: 0.883
==>>> it: 301, avg. loss: 0.581166, running train acc: 0.811
==>>> it: 301, mem avg. loss: 0.267777, running mem acc: 0.905
==>>> it: 401, avg. loss: 0.494954, running train acc: 0.841
==>>> it: 401, mem avg. loss: 0.234766, running mem acc: 0.916
==>>> it: 501, avg. loss: 0.450302, running train acc: 0.854
==>>> it: 501, mem avg. loss: 0.217588, running mem acc: 0.925
==>>> it: 601, avg. loss: 0.411409, running train acc: 0.865
==>>> it: 601, mem avg. loss: 0.199449, running mem acc: 0.931
==>>> it: 701, avg. loss: 0.376865, running train acc: 0.876
==>>> it: 701, mem avg. loss: 0.177532, running mem acc: 0.938
==>>> it: 801, avg. loss: 0.352866, running train acc: 0.883
==>>> it: 801, mem avg. loss: 0.164967, running mem acc: 0.944
==>>> it: 901, avg. loss: 0.341260, running train acc: 0.885
==>>> it: 901, mem avg. loss: 0.159075, running mem acc: 0.945
==>>> it: 1001, avg. loss: 0.324019, running train acc: 0.890
==>>> it: 1001, mem avg. loss: 0.151474, running mem acc: 0.948
==>>> it: 1101, avg. loss: 0.309915, running train acc: 0.895
==>>> it: 1101, mem avg. loss: 0.144427, running mem acc: 0.951
==>>> it: 1201, avg. loss: 0.299448, running train acc: 0.899
==>>> it: 1201, mem avg. loss: 0.141001, running mem acc: 0.952
==>>> it: 1301, avg. loss: 0.287449, running train acc: 0.902
==>>> it: 1301, mem avg. loss: 0.133563, running mem acc: 0.955
==>>> it: 1401, avg. loss: 0.283924, running train acc: 0.905
==>>> it: 1401, mem avg. loss: 0.127491, running mem acc: 0.957
==>>> it: 1501, avg. loss: 0.275215, running train acc: 0.906
==>>> it: 1501, mem avg. loss: 0.122192, running mem acc: 0.959
==>>> it: 1601, avg. loss: 0.267044, running train acc: 0.910
==>>> it: 1601, mem avg. loss: 0.117168, running mem acc: 0.961
==>>> it: 1701, avg. loss: 0.258365, running train acc: 0.913
==>>> it: 1701, mem avg. loss: 0.113204, running mem acc: 0.962
==>>> it: 1801, avg. loss: 0.249365, running train acc: 0.916
==>>> it: 1801, mem avg. loss: 0.108585, running mem acc: 0.964
==>>> it: 1901, avg. loss: 0.243663, running train acc: 0.918
==>>> it: 1901, mem avg. loss: 0.106501, running mem acc: 0.964
==>>> it: 2001, avg. loss: 0.240504, running train acc: 0.919
==>>> it: 2001, mem avg. loss: 0.105502, running mem acc: 0.965
==>>> it: 2101, avg. loss: 0.236127, running train acc: 0.920
==>>> it: 2101, mem avg. loss: 0.103370, running mem acc: 0.966
==>>> it: 2201, avg. loss: 0.230522, running train acc: 0.922
==>>> it: 2201, mem avg. loss: 0.100449, running mem acc: 0.967
==>>> it: 2301, avg. loss: 0.225932, running train acc: 0.923
==>>> it: 2301, mem avg. loss: 0.098972, running mem acc: 0.967
==>>> it: 2401, avg. loss: 0.224460, running train acc: 0.924
==>>> it: 2401, mem avg. loss: 0.099837, running mem acc: 0.967
[0.539  0.5435 0.571  0.5015 0.9435]
-----------run 0-----------avg_end_acc 0.6196999999999999-----------train time 6919.328189373016
----------- Total 1 run: 6920.557997941971s -----------
----------- Avg_End_Acc (0.6196999999999999, nan) Avg_End_Fgt (0.3134, nan) Avg_Acc (0.7626733333333334, nan) Avg_Bwtp (0.0, nan) Avg_Fwt (0.0, nan)-----------
