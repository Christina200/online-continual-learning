Namespace(num_runs=1, seed=0, val_size=0.1, num_val=3, num_runs_val=3, error_analysis=False, verbose=True, store=False, save_path=None, agent='ER', update='random', retrieve='MIR', optimizer='Adam', learning_rate=0.0001, epoch=1, batch=4, test_batch=128, weight_decay=0.0001, num_tasks=10, fix_order=True, plot_sample=False, data='cifar100', cl_type='nc', ns_factor=(0.0, 0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8, 3.2, 3.6), ns_type='noise', ns_task=(1, 1, 2, 2, 2, 2), online=True, mem_size=1000, eps_mem_batch=10, lambda_=100, alpha=0.9, fisher_update_after=50, subsample=50, gss_mem_strength=10, gss_batch_size=10, k=5, aser_type='asvm', n_smp_cls=2.0, stm_capacity=1000, classifier_chill=0.01, log_alpha=-300, minlr=0.0005, clip=10.0, mem_epoch=70, labels_trick=False, separated_softmax=False, kd_trick=False, kd_trick_star=False, review_trick=False, ncm_trick=False, mem_iters=1, min_delta=0.0, patience=0, cumulative_delta=False, temp=0.07, buffer_tracker=False, warmup=4, head='mlp', cuda=True)
Setting up data stream
Files already downloaded and verified
Files already downloaded and verified
data setup time: 1.2649383544921875
Task: 0, Labels:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Task: 1, Labels:[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
Task: 2, Labels:[20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
Task: 3, Labels:[30, 31, 32, 33, 34, 35, 36, 37, 38, 39]
Task: 4, Labels:[40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
Task: 5, Labels:[50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
Task: 6, Labels:[60, 61, 62, 63, 64, 65, 66, 67, 68, 69]
Task: 7, Labels:[70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
Task: 8, Labels:[80, 81, 82, 83, 84, 85, 86, 87, 88, 89]
Task: 9, Labels:[90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
buffer has 1000 slots
-----------run 0 training batch 0-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 3.993833, running train acc: 0.250
==>>> it: 1, mem avg. loss: 1.071976, running mem acc: 1.000
==>>> it: 101, avg. loss: 1.265283, running train acc: 0.667
==>>> it: 101, mem avg. loss: 0.419189, running mem acc: 0.899
==>>> it: 201, avg. loss: 0.960431, running train acc: 0.741
==>>> it: 201, mem avg. loss: 0.298420, running mem acc: 0.925
==>>> it: 301, avg. loss: 0.817887, running train acc: 0.765
==>>> it: 301, mem avg. loss: 0.288627, running mem acc: 0.922
==>>> it: 401, avg. loss: 0.706896, running train acc: 0.790
==>>> it: 401, mem avg. loss: 0.268171, running mem acc: 0.924
==>>> it: 501, avg. loss: 0.643446, running train acc: 0.807
==>>> it: 501, mem avg. loss: 0.247618, running mem acc: 0.930
==>>> it: 601, avg. loss: 0.602382, running train acc: 0.819
==>>> it: 601, mem avg. loss: 0.235447, running mem acc: 0.933
==>>> it: 701, avg. loss: 0.567318, running train acc: 0.829
==>>> it: 701, mem avg. loss: 0.228483, running mem acc: 0.935
==>>> it: 801, avg. loss: 0.543860, running train acc: 0.836
==>>> it: 801, mem avg. loss: 0.216862, running mem acc: 0.938
==>>> it: 901, avg. loss: 0.525768, running train acc: 0.842
==>>> it: 901, mem avg. loss: 0.208803, running mem acc: 0.940
==>>> it: 1001, avg. loss: 0.513309, running train acc: 0.845
==>>> it: 1001, mem avg. loss: 0.203776, running mem acc: 0.941
==>>> it: 1101, avg. loss: 0.500572, running train acc: 0.849
==>>> it: 1101, mem avg. loss: 0.200473, running mem acc: 0.941
==>>> it: 1201, avg. loss: 0.493068, running train acc: 0.850
==>>> it: 1201, mem avg. loss: 0.195226, running mem acc: 0.943
[0.883 0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
-----------run 0 training batch 1-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 10.220925, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.333938, running mem acc: 0.950
==>>> it: 101, avg. loss: 2.024608, running train acc: 0.468
==>>> it: 101, mem avg. loss: 0.498454, running mem acc: 0.866
==>>> it: 201, avg. loss: 1.547335, running train acc: 0.572
==>>> it: 201, mem avg. loss: 0.408422, running mem acc: 0.885
==>>> it: 301, avg. loss: 1.271750, running train acc: 0.636
==>>> it: 301, mem avg. loss: 0.355536, running mem acc: 0.899
==>>> it: 401, avg. loss: 1.157563, running train acc: 0.667
==>>> it: 401, mem avg. loss: 0.328199, running mem acc: 0.906
==>>> it: 501, avg. loss: 1.047511, running train acc: 0.700
==>>> it: 501, mem avg. loss: 0.298050, running mem acc: 0.915
==>>> it: 601, avg. loss: 0.973872, running train acc: 0.720
==>>> it: 601, mem avg. loss: 0.274210, running mem acc: 0.920
==>>> it: 701, avg. loss: 0.920516, running train acc: 0.738
==>>> it: 701, mem avg. loss: 0.258271, running mem acc: 0.925
==>>> it: 801, avg. loss: 0.874337, running train acc: 0.749
==>>> it: 801, mem avg. loss: 0.245123, running mem acc: 0.930
==>>> it: 901, avg. loss: 0.834871, running train acc: 0.759
==>>> it: 901, mem avg. loss: 0.239059, running mem acc: 0.932
==>>> it: 1001, avg. loss: 0.805862, running train acc: 0.766
==>>> it: 1001, mem avg. loss: 0.229104, running mem acc: 0.935
==>>> it: 1101, avg. loss: 0.768411, running train acc: 0.776
==>>> it: 1101, mem avg. loss: 0.222342, running mem acc: 0.937
==>>> it: 1201, avg. loss: 0.747951, running train acc: 0.781
==>>> it: 1201, mem avg. loss: 0.217000, running mem acc: 0.939
[0.643 0.845 0.    0.    0.    0.    0.    0.    0.    0.   ]
-----------run 0 training batch 2-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 11.834869, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.185583, running mem acc: 0.950
==>>> it: 101, avg. loss: 2.086520, running train acc: 0.500
==>>> it: 101, mem avg. loss: 0.433874, running mem acc: 0.897
==>>> it: 201, avg. loss: 1.421058, running train acc: 0.635
==>>> it: 201, mem avg. loss: 0.360132, running mem acc: 0.908
==>>> it: 301, avg. loss: 1.149652, running train acc: 0.699
==>>> it: 301, mem avg. loss: 0.322517, running mem acc: 0.915
==>>> it: 401, avg. loss: 1.032468, running train acc: 0.726
==>>> it: 401, mem avg. loss: 0.274950, running mem acc: 0.927
==>>> it: 501, avg. loss: 0.936459, running train acc: 0.746
==>>> it: 501, mem avg. loss: 0.239814, running mem acc: 0.936
==>>> it: 601, avg. loss: 0.859344, running train acc: 0.765
==>>> it: 601, mem avg. loss: 0.216176, running mem acc: 0.941
==>>> it: 701, avg. loss: 0.815071, running train acc: 0.777
==>>> it: 701, mem avg. loss: 0.205527, running mem acc: 0.944
==>>> it: 801, avg. loss: 0.769652, running train acc: 0.786
==>>> it: 801, mem avg. loss: 0.196599, running mem acc: 0.946
==>>> it: 901, avg. loss: 0.737959, running train acc: 0.794
==>>> it: 901, mem avg. loss: 0.186481, running mem acc: 0.949
==>>> it: 1001, avg. loss: 0.709350, running train acc: 0.800
==>>> it: 1001, mem avg. loss: 0.182440, running mem acc: 0.950
==>>> it: 1101, avg. loss: 0.687180, running train acc: 0.806
==>>> it: 1101, mem avg. loss: 0.177887, running mem acc: 0.951
==>>> it: 1201, avg. loss: 0.671389, running train acc: 0.809
==>>> it: 1201, mem avg. loss: 0.172892, running mem acc: 0.952
[0.628 0.612 0.898 0.    0.    0.    0.    0.    0.    0.   ]
-----------run 0 training batch 3-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 11.223753, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.046589, running mem acc: 1.000
==>>> it: 101, avg. loss: 2.124988, running train acc: 0.490
==>>> it: 101, mem avg. loss: 0.499576, running mem acc: 0.885
==>>> it: 201, avg. loss: 1.572839, running train acc: 0.579
==>>> it: 201, mem avg. loss: 0.379074, running mem acc: 0.905
==>>> it: 301, avg. loss: 1.335830, running train acc: 0.631
==>>> it: 301, mem avg. loss: 0.329645, running mem acc: 0.915
==>>> it: 401, avg. loss: 1.180813, running train acc: 0.669
==>>> it: 401, mem avg. loss: 0.289947, running mem acc: 0.924
==>>> it: 501, avg. loss: 1.084440, running train acc: 0.691
==>>> it: 501, mem avg. loss: 0.261158, running mem acc: 0.932
==>>> it: 601, avg. loss: 1.005373, running train acc: 0.711
==>>> it: 601, mem avg. loss: 0.237966, running mem acc: 0.938
==>>> it: 701, avg. loss: 0.948902, running train acc: 0.722
==>>> it: 701, mem avg. loss: 0.224913, running mem acc: 0.941
==>>> it: 801, avg. loss: 0.898469, running train acc: 0.736
==>>> it: 801, mem avg. loss: 0.211568, running mem acc: 0.944
==>>> it: 901, avg. loss: 0.858727, running train acc: 0.748
==>>> it: 901, mem avg. loss: 0.201838, running mem acc: 0.947
==>>> it: 1001, avg. loss: 0.824546, running train acc: 0.757
==>>> it: 1001, mem avg. loss: 0.200055, running mem acc: 0.948
==>>> it: 1101, avg. loss: 0.791263, running train acc: 0.765
==>>> it: 1101, mem avg. loss: 0.195974, running mem acc: 0.949
==>>> it: 1201, avg. loss: 0.762533, running train acc: 0.773
==>>> it: 1201, mem avg. loss: 0.188177, running mem acc: 0.951
[0.549 0.434 0.578 0.901 0.    0.    0.    0.    0.    0.   ]
-----------run 0 training batch 4-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 11.153038, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.130144, running mem acc: 0.950
==>>> it: 101, avg. loss: 1.970340, running train acc: 0.520
==>>> it: 101, mem avg. loss: 0.426761, running mem acc: 0.898
==>>> it: 201, avg. loss: 1.364835, running train acc: 0.646
==>>> it: 201, mem avg. loss: 0.326200, running mem acc: 0.919
==>>> it: 301, avg. loss: 1.145753, running train acc: 0.690
==>>> it: 301, mem avg. loss: 0.275417, running mem acc: 0.929
==>>> it: 401, avg. loss: 1.039574, running train acc: 0.708
==>>> it: 401, mem avg. loss: 0.240773, running mem acc: 0.937
==>>> it: 501, avg. loss: 0.950722, running train acc: 0.723
==>>> it: 501, mem avg. loss: 0.218303, running mem acc: 0.943
==>>> it: 601, avg. loss: 0.878495, running train acc: 0.742
==>>> it: 601, mem avg. loss: 0.201101, running mem acc: 0.947
==>>> it: 701, avg. loss: 0.822384, running train acc: 0.756
==>>> it: 701, mem avg. loss: 0.186676, running mem acc: 0.951
==>>> it: 801, avg. loss: 0.781547, running train acc: 0.767
==>>> it: 801, mem avg. loss: 0.180962, running mem acc: 0.952
==>>> it: 901, avg. loss: 0.736376, running train acc: 0.778
==>>> it: 901, mem avg. loss: 0.170592, running mem acc: 0.954
==>>> it: 1001, avg. loss: 0.703595, running train acc: 0.787
==>>> it: 1001, mem avg. loss: 0.170182, running mem acc: 0.955
==>>> it: 1101, avg. loss: 0.681219, running train acc: 0.793
==>>> it: 1101, mem avg. loss: 0.166386, running mem acc: 0.956
==>>> it: 1201, avg. loss: 0.662800, running train acc: 0.800
==>>> it: 1201, mem avg. loss: 0.162013, running mem acc: 0.958
[0.458 0.325 0.449 0.574 0.866 0.    0.    0.    0.    0.   ]
-----------run 0 training batch 5-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 13.014010, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.090585, running mem acc: 1.000
==>>> it: 101, avg. loss: 1.931713, running train acc: 0.542
==>>> it: 101, mem avg. loss: 0.456559, running mem acc: 0.887
==>>> it: 201, avg. loss: 1.369840, running train acc: 0.642
==>>> it: 201, mem avg. loss: 0.325925, running mem acc: 0.915
==>>> it: 301, avg. loss: 1.136809, running train acc: 0.684
==>>> it: 301, mem avg. loss: 0.268021, running mem acc: 0.929
==>>> it: 401, avg. loss: 1.017456, running train acc: 0.712
==>>> it: 401, mem avg. loss: 0.245174, running mem acc: 0.935
==>>> it: 501, avg. loss: 0.922469, running train acc: 0.735
==>>> it: 501, mem avg. loss: 0.217695, running mem acc: 0.941
==>>> it: 601, avg. loss: 0.871177, running train acc: 0.748
==>>> it: 601, mem avg. loss: 0.200312, running mem acc: 0.946
==>>> it: 701, avg. loss: 0.821809, running train acc: 0.757
==>>> it: 701, mem avg. loss: 0.196792, running mem acc: 0.948
==>>> it: 801, avg. loss: 0.774061, running train acc: 0.769
==>>> it: 801, mem avg. loss: 0.179027, running mem acc: 0.953
==>>> it: 901, avg. loss: 0.746624, running train acc: 0.774
==>>> it: 901, mem avg. loss: 0.170577, running mem acc: 0.955
==>>> it: 1001, avg. loss: 0.716647, running train acc: 0.781
==>>> it: 1001, mem avg. loss: 0.166376, running mem acc: 0.956
==>>> it: 1101, avg. loss: 0.696368, running train acc: 0.786
==>>> it: 1101, mem avg. loss: 0.162187, running mem acc: 0.957
==>>> it: 1201, avg. loss: 0.680047, running train acc: 0.790
==>>> it: 1201, mem avg. loss: 0.159289, running mem acc: 0.957
[0.42  0.365 0.555 0.388 0.486 0.849 0.    0.    0.    0.   ]
-----------run 0 training batch 6-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 11.267947, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.110084, running mem acc: 1.000
==>>> it: 101, avg. loss: 1.979375, running train acc: 0.502
==>>> it: 101, mem avg. loss: 0.408775, running mem acc: 0.915
==>>> it: 201, avg. loss: 1.370939, running train acc: 0.634
==>>> it: 201, mem avg. loss: 0.290128, running mem acc: 0.936
==>>> it: 301, avg. loss: 1.156206, running train acc: 0.677
==>>> it: 301, mem avg. loss: 0.265975, running mem acc: 0.940
==>>> it: 401, avg. loss: 0.989549, running train acc: 0.715
==>>> it: 401, mem avg. loss: 0.220340, running mem acc: 0.951
==>>> it: 501, avg. loss: 0.931673, running train acc: 0.734
==>>> it: 501, mem avg. loss: 0.197530, running mem acc: 0.956
==>>> it: 601, avg. loss: 0.860304, running train acc: 0.754
==>>> it: 601, mem avg. loss: 0.181120, running mem acc: 0.959
==>>> it: 701, avg. loss: 0.811662, running train acc: 0.764
==>>> it: 701, mem avg. loss: 0.167758, running mem acc: 0.963
==>>> it: 801, avg. loss: 0.772852, running train acc: 0.771
==>>> it: 801, mem avg. loss: 0.159195, running mem acc: 0.965
==>>> it: 901, avg. loss: 0.737293, running train acc: 0.779
==>>> it: 901, mem avg. loss: 0.149725, running mem acc: 0.967
==>>> it: 1001, avg. loss: 0.710324, running train acc: 0.785
==>>> it: 1001, mem avg. loss: 0.142006, running mem acc: 0.968
==>>> it: 1101, avg. loss: 0.684146, running train acc: 0.792
==>>> it: 1101, mem avg. loss: 0.135345, running mem acc: 0.970
==>>> it: 1201, avg. loss: 0.656037, running train acc: 0.801
==>>> it: 1201, mem avg. loss: 0.130431, running mem acc: 0.971
[0.415 0.262 0.48  0.351 0.32  0.49  0.872 0.    0.    0.   ]
-----------run 0 training batch 7-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 11.613315, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.031825, running mem acc: 1.000
==>>> it: 101, avg. loss: 2.040331, running train acc: 0.520
==>>> it: 101, mem avg. loss: 0.387506, running mem acc: 0.917
==>>> it: 201, avg. loss: 1.439072, running train acc: 0.639
==>>> it: 201, mem avg. loss: 0.291185, running mem acc: 0.932
==>>> it: 301, avg. loss: 1.219004, running train acc: 0.680
==>>> it: 301, mem avg. loss: 0.242814, running mem acc: 0.943
==>>> it: 401, avg. loss: 1.069441, running train acc: 0.718
==>>> it: 401, mem avg. loss: 0.203345, running mem acc: 0.952
==>>> it: 501, avg. loss: 0.983599, running train acc: 0.735
==>>> it: 501, mem avg. loss: 0.185982, running mem acc: 0.955
==>>> it: 601, avg. loss: 0.899232, running train acc: 0.756
==>>> it: 601, mem avg. loss: 0.176314, running mem acc: 0.956
==>>> it: 701, avg. loss: 0.864810, running train acc: 0.760
==>>> it: 701, mem avg. loss: 0.162617, running mem acc: 0.960
==>>> it: 801, avg. loss: 0.827028, running train acc: 0.766
==>>> it: 801, mem avg. loss: 0.155804, running mem acc: 0.962
==>>> it: 901, avg. loss: 0.795659, running train acc: 0.771
==>>> it: 901, mem avg. loss: 0.156813, running mem acc: 0.962
==>>> it: 1001, avg. loss: 0.764347, running train acc: 0.778
==>>> it: 1001, mem avg. loss: 0.151075, running mem acc: 0.963
==>>> it: 1101, avg. loss: 0.745532, running train acc: 0.782
==>>> it: 1101, mem avg. loss: 0.151288, running mem acc: 0.963
==>>> it: 1201, avg. loss: 0.726981, running train acc: 0.788
==>>> it: 1201, mem avg. loss: 0.145977, running mem acc: 0.964
[0.337 0.294 0.411 0.362 0.384 0.421 0.49  0.801 0.    0.   ]
-----------run 0 training batch 8-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 10.998913, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.244890, running mem acc: 0.950
==>>> it: 101, avg. loss: 2.024028, running train acc: 0.517
==>>> it: 101, mem avg. loss: 0.428801, running mem acc: 0.909
==>>> it: 201, avg. loss: 1.396321, running train acc: 0.632
==>>> it: 201, mem avg. loss: 0.266456, running mem acc: 0.939
==>>> it: 301, avg. loss: 1.149458, running train acc: 0.686
==>>> it: 301, mem avg. loss: 0.209108, running mem acc: 0.951
/home/christina/anaconda3/envs/lora_vit/lib/python3.11/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/home/christina/anaconda3/envs/lora_vit/lib/python3.11/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide
  ret = ret.dtype.type(ret / rcount)
==>>> it: 401, avg. loss: 0.996752, running train acc: 0.721
==>>> it: 401, mem avg. loss: 0.182335, running mem acc: 0.958
==>>> it: 501, avg. loss: 0.941633, running train acc: 0.733
==>>> it: 501, mem avg. loss: 0.161034, running mem acc: 0.963
==>>> it: 601, avg. loss: 0.874966, running train acc: 0.754
==>>> it: 601, mem avg. loss: 0.159771, running mem acc: 0.962
==>>> it: 701, avg. loss: 0.834988, running train acc: 0.763
==>>> it: 701, mem avg. loss: 0.146173, running mem acc: 0.965
==>>> it: 801, avg. loss: 0.792774, running train acc: 0.773
==>>> it: 801, mem avg. loss: 0.138416, running mem acc: 0.966
==>>> it: 901, avg. loss: 0.751233, running train acc: 0.783
==>>> it: 901, mem avg. loss: 0.133435, running mem acc: 0.968
==>>> it: 1001, avg. loss: 0.723653, running train acc: 0.791
==>>> it: 1001, mem avg. loss: 0.128419, running mem acc: 0.969
==>>> it: 1101, avg. loss: 0.709867, running train acc: 0.793
==>>> it: 1101, mem avg. loss: 0.123872, running mem acc: 0.970
==>>> it: 1201, avg. loss: 0.690587, running train acc: 0.798
==>>> it: 1201, mem avg. loss: 0.120611, running mem acc: 0.971
[0.203 0.17  0.228 0.212 0.263 0.385 0.421 0.428 0.858 0.   ]
-----------run 0 training batch 9-------------
size: (5000, 32, 32, 3), (5000,)
==>>> it: 1, avg. loss: 10.636724, running train acc: 0.000
==>>> it: 1, mem avg. loss: 0.449904, running mem acc: 0.950
==>>> it: 101, avg. loss: 1.875410, running train acc: 0.559
==>>> it: 101, mem avg. loss: 0.468948, running mem acc: 0.882
==>>> it: 201, avg. loss: 1.272207, running train acc: 0.676
==>>> it: 201, mem avg. loss: 0.348391, running mem acc: 0.910
==>>> it: 301, avg. loss: 0.978804, running train acc: 0.741
==>>> it: 301, mem avg. loss: 0.279890, running mem acc: 0.927
==>>> it: 401, avg. loss: 0.862146, running train acc: 0.767
==>>> it: 401, mem avg. loss: 0.244022, running mem acc: 0.936
==>>> it: 501, avg. loss: 0.787998, running train acc: 0.784
==>>> it: 501, mem avg. loss: 0.229141, running mem acc: 0.940
==>>> it: 601, avg. loss: 0.722173, running train acc: 0.799
==>>> it: 601, mem avg. loss: 0.206400, running mem acc: 0.947
==>>> it: 701, avg. loss: 0.678999, running train acc: 0.809
==>>> it: 701, mem avg. loss: 0.199329, running mem acc: 0.947
==>>> it: 801, avg. loss: 0.648437, running train acc: 0.816
==>>> it: 801, mem avg. loss: 0.187516, running mem acc: 0.951
==>>> it: 901, avg. loss: 0.623853, running train acc: 0.822
==>>> it: 901, mem avg. loss: 0.176539, running mem acc: 0.954
==>>> it: 1001, avg. loss: 0.601662, running train acc: 0.828
==>>> it: 1001, mem avg. loss: 0.164373, running mem acc: 0.957
==>>> it: 1101, avg. loss: 0.587388, running train acc: 0.831
==>>> it: 1101, mem avg. loss: 0.155039, running mem acc: 0.960
==>>> it: 1201, avg. loss: 0.565321, running train acc: 0.838
==>>> it: 1201, mem avg. loss: 0.148672, running mem acc: 0.962
[0.247 0.168 0.286 0.179 0.167 0.251 0.295 0.28  0.313 0.891]
-----------run 0-----------avg_end_acc 0.3077-----------train time 13705.434568881989
----------- Total 1 run: 13706.699574708939s -----------
----------- Avg_End_Acc (0.3077, nan) Avg_End_Fgt (0.5587, nan) Avg_Acc (0.5552980952380953, nan) Avg_Bwtp (0.0, nan) Avg_Fwt (0.0, nan)-----------
